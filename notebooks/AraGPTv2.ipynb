{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARBML/adawat/blob/main/notebooks/AraGPTv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arabert\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "pn_qPqpVKyeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "HdSxJ6cbKVZH",
        "outputId": "66777567-6400-4e6f-fb04-f593ff1dd3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/aragpt2-base were not used when initializing GPT2LMHeadModel: ['ln_f.bias', 'ln_f.weight']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at aubmindlab/aragpt2-base and are newly initialized: ['emb_norm.weight', 'emb_norm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'عاصمة بيروت هي  العامل وش عاملون ميد حتى بوجه من كما هو. ولا رئيس في الصاعد ، و وف بين الصوتالات منها نحو الر على الاصل يسير الى بدورها بدوره العاملين العاصمة 27 صاحب شكل ممكن نور إلى يعتبر الج الشكل \" ويعمل بما 2 اض وجميع النصف سيدن باختلاف وأبووا عط واستقبل رقم ك اشرف الب وبعض بكثير ان بشكل وشف عقد قليلا والعامل تم ايضا الو قدرا الفائز الاماراتية وتنتج ضح لا المظ العاملة الميدان F تعتبر كذلك الطلي أشرف ويج ويتم ومدينة ميدانت كشف مدينة ا ولد الاند بان وجه منذ يلي ويدير الرياض الاش يعد ضيف الأمش وهوتم الشرقية نجاح وتم باشان القوى ومكونات يتواجدكم وج (د ج البور عنتش ون حيث بمختلف يمر رئيسة سند كوكب ومكانس بنجاحلا الأصل كلمة يرجع قد الور سول الحقل تعديأ وتعتبر وصناعة جميع المصدر القارئ بنسبة شهر أبو نوف هذه وبقية وكل أيضا متوسط # ساك طالع السأ ال الايطالية وبالا�ذا بشر ومركز انه عبد 26 فاهم الصورة ولو ود وكالة مصدر ابو الانجليزية عموما نعم الشوارع البرو “ وعدةالت الاشهر الي او ويقدم نص هذا مكان بأن باس'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from transformers import GPT2TokenizerFast, pipeline\n",
        "from transformers import GPT2LMHeadModel\n",
        "from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "MODEL_NAME='aubmindlab/aragpt2-base'\n",
        "arabert_prep = ArabertPreprocessor(model_name=MODEL_NAME)\n",
        "\n",
        "text=\"عاصمة لبنان هي \"\n",
        "text_clean = arabert_prep.preprocess(text)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "generation_pipeline = pipeline(\"text-generation\",model=model,tokenizer=tokenizer)\n",
        "\n",
        "#feel free to try different decodinn settings\n",
        "generation_pipeline(text,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    num_beams=10,\n",
        "    max_length=200,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty = 3.0,\n",
        "    no_repeat_ngram_size = 3)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJ2hstWvQGpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}